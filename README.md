# ğŸ  Airbnb End-to-End Data Engineering Project

## ğŸ“‹ Overview

This project implements a complete end-to-end data engineering pipeline for Airbnb data using modern cloud technologies. The solution demonstrates best practices in data warehousing, transformation, and analytics using **Snowflake**, **dbt (Data Build Tool)**, and **AWS**.

The pipeline processes Airbnb listings, bookings, and hosts data through a medallion architecture (Bronze â†’ Silver â†’ Gold), implementing incremental loading, slowly changing dimensions (SCD Type 2), and creating analytics-ready datasets.

## ğŸ—ï¸ Architecture

### Data Flow
```
Source Data (CSV) â†’ AWS S3 â†’ Snowflake (Staging) â†’ Bronze Layer â†’ Silver Layer â†’ Gold Layer
                                                           â†“              â†“           â†“
                                                      Raw Tables    Cleaned Data   Analytics
```

### Technology Stack

- **Cloud Data Warehouse**: Snowflake
- **Transformation Layer**: dbt (Data Build Tool)
- **Cloud Storage**: AWS S3 (implied)
- **Version Control**: Git
- **Python**: 3.12+
- **Key dbt Features**:
  - Incremental models
  - Snapshots (SCD Type 2)
  - Custom macros
  - Jinja templating
  - Testing and documentation

## ğŸ“Š Data Model

### Medallion Architecture

#### ğŸ¥‰ Bronze Layer (Raw Data)
Raw data ingested from staging with minimal transformations:
- `bronze_bookings` - Raw booking transactions
- `bronze_hosts` - Raw host information
- `bronze_listings` - Raw property listings

#### ğŸ¥ˆ Silver Layer (Cleaned Data)
Cleaned and standardized data:
- `silver_bookings` - Validated booking records
- `silver_hosts` - Enhanced host profiles with quality metrics
- `silver_listings` - Standardized listing information with price categorization

#### ğŸ¥‡ Gold Layer (Analytics-Ready)
Business-ready datasets optimized for analytics:
- `obt` (One Big Table) - Denormalized fact table joining bookings, listings, and hosts
- `fact` - Fact table for dimensional modeling
- Ephemeral models for intermediate transformations

### Snapshots (SCD Type 2)
Slowly Changing Dimensions to track historical changes:
- `dim_bookings` - Historical booking changes
- `dim_hosts` - Historical host profile changes
- `dim_listings` - Historical listing changes

## ğŸ“ Project Structure

```
AWS_DBT_Snowflake/
â”œâ”€â”€ README.md                           # This file
â”œâ”€â”€ pyproject.toml                      # Python dependencies
â”œâ”€â”€ main.py                             # Main execution script
â”‚
â”œâ”€â”€ SourceData/                         # Raw CSV data files
â”‚   â”œâ”€â”€ bookings.csv
â”‚   â”œâ”€â”€ hosts.csv
â”‚   â””â”€â”€ listings.csv
â”‚
â”œâ”€â”€ DDL/                                # Database schema definitions
â”‚   â”œâ”€â”€ ddl.sql                         # Table creation scripts
â”‚   â””â”€â”€ resources.sql
â”‚
â””â”€â”€ aws_dbt_snowflake_project/         # Main dbt project
    â”œâ”€â”€ dbt_project.yml                 # dbt project configuration
    â”œâ”€â”€ ExampleProfiles.yml             # Snowflake connection profile
    â”‚
    â”œâ”€â”€ models/                         # dbt models
    â”‚   â”œâ”€â”€ sources/
    â”‚   â”‚   â””â”€â”€ sources.yml             # Source definitions
    â”‚   â”œâ”€â”€ bronze/                     # Raw data layer
    â”‚   â”‚   â”œâ”€â”€ bronze_bookings.sql
    â”‚   â”‚   â”œâ”€â”€ bronze_hosts.sql
    â”‚   â”‚   â””â”€â”€ bronze_listings.sql
    â”‚   â”œâ”€â”€ silver/                     # Cleaned data layer
    â”‚   â”‚   â”œâ”€â”€ silver_bookings.sql
    â”‚   â”‚   â”œâ”€â”€ silver_hosts.sql
    â”‚   â”‚   â””â”€â”€ silver_listings.sql
    â”‚   â””â”€â”€ gold/                       # Analytics layer
    â”‚       â”œâ”€â”€ fact.sql
    â”‚       â”œâ”€â”€ obt.sql
    â”‚       â””â”€â”€ ephemeral/              # Temporary models
    â”‚           â”œâ”€â”€ bookings.sql
    â”‚           â”œâ”€â”€ hosts.sql
    â”‚           â””â”€â”€ listings.sql
    â”‚
    â”œâ”€â”€ macros/                         # Reusable SQL functions
    â”‚   â”œâ”€â”€ generate_schema_name.sql    # Custom schema naming
    â”‚   â”œâ”€â”€ multiply.sql                # Math operations
    â”‚   â”œâ”€â”€ tag.sql                     # Categorization logic
    â”‚   â””â”€â”€ trimmer.sql                 # String utilities
    â”‚
    â”œâ”€â”€ analyses/                       # Ad-hoc analysis queries
    â”‚   â”œâ”€â”€ explore.sql
    â”‚   â”œâ”€â”€ if_else.sql
    â”‚   â””â”€â”€ loop.sql
    â”‚
    â”œâ”€â”€ snapshots/                      # SCD Type 2 configurations
    â”‚   â”œâ”€â”€ dim_bookings.yml
    â”‚   â”œâ”€â”€ dim_hosts.yml
    â”‚   â””â”€â”€ dim_listings.yml
    â”‚
    â”œâ”€â”€ tests/                          # Data quality tests
    â”‚   â””â”€â”€ source_tests.sql
    â”‚
    â””â”€â”€ seeds/                          # Static reference data
```

## ğŸš€ Getting Started

### Prerequisites

1. **Snowflake Account (will create one if doesn't exist)**

2. **Python Environment**
   - Python 3.12 or higher
   - pip or uv package manager

3. **AWS Account (will create one if doesn't exist) ** (for S3 storage)

### Installation

1. **Clone the Repository**
   ```bash
   git clone <repository-url>
   cd AWS_DBT_Snowflake
   ```

2. **Create Virtual Environment**
   ```bash
   python -m venv .venv
   .venv\Scripts\Activate.ps1  # Windows PowerShell
   # or
   source .venv/bin/activate    # Linux/Mac
   ```

3. **Install Dependencies**
   ```bash
   pip install -r requirements.txt
   # or using pyproject.toml
   pip install -e .
   ```

   **Core Dependencies:**
   - `dbt-core>=1.11.2`
   - `dbt-snowflake>=1.11.0`
   - `sqlfmt>=0.0.3`

4. **Configure Snowflake Connection**
   
   Create `~/.dbt/profiles.yml`:
   ```yaml
   aws_dbt_snowflake_project:
     outputs:
       dev:
         account: <your-account-identifier>
         database: AIRBNB
         password: <your-password>
         role: ACCOUNTADMIN
         schema: dbt_schema
         threads: 4
         type: snowflake
         user: <your-username>
         warehouse: COMPUTE_WH
     target: dev
   ```

5. **Set Up Snowflake Database**
   
   Run the DDL scripts to create tables:
   ```bash
   # Execute DDL/ddl.sql in Snowflake to create staging tables
   ```

6. **Load Source Data**
   
   Load CSV files from `SourceData/` to Snowflake staging schema:
   - `bookings.csv` â†’ `AIRBNB.STAGING.BOOKINGS`
   - `hosts.csv` â†’ `AIRBNB.STAGING.HOSTS`
   - `listings.csv` â†’ `AIRBNB.STAGING.LISTINGS`

## ğŸ”§ Usage

### Running dbt Commands

1. **Test Connection**
   ```bash
   cd aws_dbt_snowflake_project
   dbt debug
   ```

2. **Install Dependencies**
   ```bash
   dbt deps
   ```

3. **Run All Models**
   ```bash
   dbt run
   ```

4. **Run Specific Layer**
   ```bash
   dbt run --select bronze.*      # Run bronze models only
   dbt run --select silver.*      # Run silver models only
   dbt run --select gold.*        # Run gold models only
   ```

5. **Run Tests**
   ```bash
   dbt test
   ```

6. **Run Snapshots**
   ```bash
   dbt snapshot
   ```

7. **Generate Documentation**
   ```bash
   dbt docs generate
   dbt docs serve
   ```

8. **Build Everything**
   ```bash
   dbt build  # Runs models, tests, and snapshots
   ```

## ğŸ¯ Key Features

### 1. Incremental Loading
Bronze and silver models use incremental materialization to process only new/changed data:
```sql
{{ config(materialized='incremental') }}
{% if is_incremental() %}
    WHERE CREATED_AT > (SELECT COALESCE(MAX(CREATED_AT), '1900-01-01') FROM {{ this }})
{% endif %}
```

### 2. Custom Macros
Reusable business logic:
- **`tag()` macro**: Categorizes prices into 'low', 'medium', 'high'
  ```sql
  {{ tag('CAST(PRICE_PER_NIGHT AS INT)') }} AS PRICE_PER_NIGHT_TAG
  ```

### 3. Dynamic SQL Generation
The OBT (One Big Table) model uses Jinja loops for maintainable joins:
```sql
{% set configs = [...] %}
SELECT {% for config in configs %}...{% endfor %}
```

### 4. Slowly Changing Dimensions
Track historical changes with timestamp-based snapshots:
- Valid from/to dates automatically maintained
- Historical data preserved for point-in-time analysis

### 5. Schema Organization
Automatic schema separation by layer:
- Bronze models â†’ `AIRBNB.BRONZE.*`
- Silver models â†’ `AIRBNB.SILVER.*`
- Gold models â†’ `AIRBNB.GOLD.*`

## ğŸ“ˆ Data Quality

### Testing Strategy
- Source data validation tests
- Unique key constraints
- Not null checks
- Referential integrity tests
- Custom business rule tests

### Data Lineage
dbt automatically tracks data lineage, showing:
- Upstream dependencies
- Downstream impacts
- Model relationships
- Source to consumption flow

## ğŸ” Security & Best Practices

1. **Credentials Management**
   - Never commit `profiles.yml` with credentials
   - Use environment variables for sensitive data
   - Implement role-based access control (RBAC) in Snowflake

2. **Code Quality**
   - SQL formatting with `sqlfmt`
   - Version control with Git
   - Code reviews for model changes

3. **Performance Optimization**
   - Incremental models for large datasets
   - Ephemeral models for intermediate transformations
   - Appropriate clustering keys in Snowflake

## ğŸ“š Additional Resources

- **dbt Documentation**: https://docs.getdbt.com/
- **Snowflake Documentation**: https://docs.snowflake.com/
- **dbt Best Practices**: https://docs.getdbt.com/guides/best-practices

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

## ğŸ“ License

This project is part of a data engineering portfolio demonstration.

## ğŸ‘¤ Author

**Project**: Airbnb Data Engineering Pipeline  
**Technologies**: Snowflake, dbt, AWS, Python

## ğŸ› Troubleshooting

### Common Issues

1. **Connection Error**
   - Verify Snowflake credentials in `profiles.yml`
   - Check network connectivity
   - Ensure warehouse is running

2. **Compilation Error**
   - Run `dbt debug` to check configuration
   - Verify model dependencies
   - Check Jinja syntax

3. **Incremental Load Issues**
   - Run `dbt run --full-refresh` to rebuild from scratch
   - Verify source data timestamps

## ğŸ“Š Future Enhancements

- [ ] Add data quality dashboards
- [ ] Implement CI/CD pipeline
- [ ] Add more complex business metrics
- [ ] Integrate with BI tools (Tableau/Power BI)
- [ ] Add alerting and monitoring
- [ ] Implement data masking for PII
- [ ] Add more comprehensive testing suite
